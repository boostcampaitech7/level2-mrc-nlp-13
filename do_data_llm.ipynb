{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (BartForConditionalGeneration,\n",
    "                          PreTrainedTokenizerFast)\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터를 토큰화하고 입력으로 사용할 수 있도록 변환하는 클래스\n",
    "class DatasetFromDataframe(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_len=512) -> None:\n",
    "        self.data = df\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # 입력 텍스트를 토큰 ID로 변환하고, 이를 길이에 맞게 처리한 후 입력 ID와 어텐션 마스크를 생성\n",
    "    def make_input_id_mask(self, tokens, index):\n",
    "        input_id = self.tokenizer.convert_tokens_to_ids(tokens) # 텍스트를 토크나이저로 토큰화한 후, 이를 토큰 ID로 변환한 리스트\n",
    "        attention_mask = [1] * len(input_id) # 토큰 ID 리스트에서 실제 토큰에 해당하는 위치는 1, 패딩 위치는 0으로 채워진 리스트\n",
    "        # 입력 시퀀스가 max_seq_len보다 짧으면 <pad> 토큰을 추가하고, 더 길면 자르고 마지막에 </s> (종료 토큰)를 추가\n",
    "        if len(input_id) < self.max_seq_len:\n",
    "            while len(input_id) < self.max_seq_len:\n",
    "                input_id += [self.tokenizer.pad_token_id]\n",
    "                attention_mask += [0]\n",
    "        else:\n",
    "            print(f'exceed max_seq_len for given article : {index}')\n",
    "            input_id = input_id[:self.max_seq_len - 1] + [ self.tokenizer.eos_token_id ]\n",
    "            attention_mask = attention_mask[:self.max_seq_len]\n",
    "        return input_id, attention_mask\n",
    "\n",
    "    # 특정 인덱스의 데이터를 반환하는 역할\n",
    "    def __getitem__(self, index):\n",
    "        record = self.data.iloc[index]\n",
    "        \n",
    "        # 텍스트 앞뒤에 <s> (시작)와 </s> (종료) 토큰을 추가한 후, make_input_id_mask를 사용해 입력 ID와 어텐션 마스크를 생성\n",
    "        if 'input_id' in record.keys():\n",
    "            q, a = record['input_id'], record['target_id']\n",
    "\n",
    "            q_tokens = [ self.bos_token ] + q + [ self.eos_token ]\n",
    "            a_tokens = [ self.bos_token ] + a + [ self.eos_token ]\n",
    "\n",
    "        else:\n",
    "            q, a = record['input_text'], record['target_text']\n",
    "\n",
    "            q_tokens = [ self.bos_token ] + self.tokenizer.tokenize(q) + [ self.eos_token ]\n",
    "            a_tokens = [ self.bos_token ] + self.tokenizer.tokenize(a) + [ self.eos_token ]\n",
    "        \n",
    "        encoder_input_id, encoder_attention_mask = self.make_input_id_mask(q_tokens, index)\n",
    "        decoder_input_id, decoder_attention_mask = self.make_input_id_mask(a_tokens, index)\n",
    "        \n",
    "        # labels: 디코더의 타겟 시퀀스를 나타내며, 손실 계산 시 사용된다.\n",
    "        # 마찬가지로 패딩을 -100으로 설정해 교차 엔트로피 손실을 계산할 때 무시할 수 있도록 한다.\n",
    "        labels = self.tokenizer.convert_tokens_to_ids(\n",
    "            a_tokens[1:(self.max_seq_len + 1)]\n",
    "        )\n",
    "\n",
    "        # WTF is this??\n",
    "        if len(labels) < self.max_seq_len:\n",
    "            while len(labels) < self.max_seq_len:\n",
    "                # for cross entropy loss masking\n",
    "                labels += [-100]\n",
    "\n",
    "        # 텐서로 변환된 데이터를 반환\n",
    "        return {\n",
    "            'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
    "            'attention_mask': np.array(encoder_attention_mask, dtype=np.float_),\n",
    "            'decoder_input_ids': np.array(decoder_input_id, dtype=np.int_),\n",
    "            'decoder_attention_mask': np.array(decoder_attention_mask, dtype=np.float_),\n",
    "            'labels': np.array(labels, dtype=np.int_)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 파일 경로나 데이터프레임을 바탕으로 데이터를 불러오고,\n",
    "# 이를 DataLoader로 묶어 학습, 검증, 테스트 시에 사용할 수 있도록 준비하는 역할\n",
    "class OneSourceDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath,\n",
    "        custom_dataset,\n",
    "        tokenizer,\n",
    "        max_length = 36,\n",
    "        batch_size = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if type(filepath) is str:\n",
    "            self.filepath = filepath\n",
    "            self.data = False\n",
    "        else:\n",
    "            self.filepath = False\n",
    "            self.data = filepath\n",
    "        \n",
    "        self.custom_dataset = custom_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.num_workers = 2\n",
    "        self.train_size = 0.9\n",
    "\n",
    "    # 이 함수는 데이터셋을 train과 test으로 나누고, 각 세트를 custom_dataset 형식으로 변환\n",
    "    def setup(self, stage=\"\"):\n",
    "        df = pd.read_csv(self.filepath) if self.filepath else self.data\n",
    "        trainset, testset = train_test_split(df, train_size=self.train_size, shuffle=True)\n",
    "        \n",
    "        self.trainset = self.custom_dataset(trainset, self.tokenizer, self.max_seq_len)\n",
    "        self.testset = self.custom_dataset(testset, self.tokenizer, self.max_seq_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train = DataLoader(\n",
    "            self.trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val = DataLoader(\n",
    "            self.testset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "        return val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test = DataLoader(\n",
    "            self.testset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "        return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터를 최적화하고, 학습이 진행됨에 따라 학습률을 조절하는 작업\n",
    "class Base(pl.LightningModule):\n",
    "    def __init__(self, hparams, **kwargs) -> None:\n",
    "        super(Base, self).__init__()\n",
    "        self.hparams.update(hparams)\n",
    "\n",
    "    # 옵티마이저와 학습률 스케줄러를 설정하는 역할\n",
    "    def configure_optimizers(self):\n",
    "        # Prepare optimizer\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight'] # no_decay: 가중치 감쇠(Weight Decay)를 적용하지 않을 파라미터들\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(\n",
    "                   nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                        lr=self.hparams.lr, correct_bias=False)\n",
    "        # warm up lr\n",
    "        num_workers = (self.hparams.gpus if self.hparams.gpus is not None else 1) * (self.hparams.num_nodes if self.hparams.num_nodes is not None else 1)\n",
    "        data_len = len(self.train_dataloader().dataset)\n",
    "        print(f'number of workers {num_workers}, data length {data_len}')\n",
    "        num_train_steps = int(data_len / (self.hparams.batch_size * num_workers) * self.hparams.max_epochs)\n",
    "        print(f'num_train_steps : {num_train_steps}')\n",
    "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
    "        print(f'num_warmup_steps : {num_warmup_steps}')\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "        lr_scheduler = {'scheduler': scheduler, \n",
    "                        'monitor': 'loss', 'interval': 'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBART 모델을 이용한 조건부 생성(Conditional Generation) 모델을 정의\n",
    "class KoBARTConditionalGeneration(Base):\n",
    "    def __init__(self, hparams, **kwargs):\n",
    "        super(KoBARTConditionalGeneration, self).__init__(hparams, **kwargs)\n",
    "        \n",
    "        self.model = kwargs['model']\n",
    "        self.tokenizer = kwargs['tokenizer']\n",
    "        self.model.train()\n",
    "        \n",
    "        self.bos_token = tokenizer.bos_token\n",
    "        self.eos_token = tokenizer.eos_token\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            decoder_input_ids=inputs['decoder_input_ids'],\n",
    "            decoder_attention_mask=inputs['decoder_attention_mask'],\n",
    "            labels=inputs['labels'], return_dict=True\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outs = self(batch)\n",
    "        loss = outs.loss\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    # training_step과 동일한 방식으로 순전파를 수행하고, 손실 값을 계산\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outs = self(batch)\n",
    "        loss = outs['loss']\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    # 대화 생성 함수\n",
    "    def chat(self, text):\n",
    "        input_ids =  [self.tokenizer.bos_token_id] + self.tokenizer.encode(text) + [self.tokenizer.eos_token_id]\n",
    "        res_ids = self.model.generate(\n",
    "            torch.tensor([input_ids]),\n",
    "            max_length=self.hparams.max_seq_len,\n",
    "            num_beams=6,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            bad_words_ids=[[self.tokenizer.unk_token_id]]\n",
    "        )\n",
    "        a = self.tokenizer.batch_decode(res_ids.tolist())[0]\n",
    "        return a.replace('<s>', '').replace('</s>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextPairFromWiki(wiki_df):\n",
    "    # 컬럼명 정의\n",
    "    COLUMN_NAMES = ['input_text', 'target_text']\n",
    "    \n",
    "    # 질문-답변 쌍 생성\n",
    "    questionPair = pd.DataFrame([\n",
    "        [\n",
    "            ans['text'][0] + \"<unused0>\" + row['context'],  # 답변 + 문맥\n",
    "            row['question']  # 질문\n",
    "        ]\n",
    "        for _, row in wiki_df.iterrows() if len(row['answers']['text']) > 0\n",
    "        for ans in [row['answers']]  # 답변 추출\n",
    "    ], columns=COLUMN_NAMES)\n",
    "    \n",
    "    # 키워드 추출용 데이터 생성\n",
    "    keywordPair = pd.DataFrame([\n",
    "        [\n",
    "            row['context'],  # 문맥\n",
    "            list(set([text for text in ans['text'] if len(ans['text']) > 0]))  # 중복 제거한 키워드(답변들)\n",
    "        ]\n",
    "        for _, row in wiki_df.iterrows()\n",
    "        for ans in [row['answers']]  # 답변 추출\n",
    "    ], columns=COLUMN_NAMES)\n",
    "    \n",
    "    # 키워드 출력 추가\n",
    "    # for _, row in wiki_df.iterrows():\n",
    "    #     keywords = list(set([ans['text'][0] for ans in [row['answers']] if len(ans['text']) > 0]))\n",
    "    #     print(f\"키워드: {keywords}\")\n",
    "    \n",
    "    # 키워드의 개수를 계산하여 3개 이상인 문단만 필터링 => 기각\n",
    "    keywordCounts = keywordPair['target_text'].apply(len)\n",
    "    # keywordPair = keywordPair[keywordCounts > 3]\n",
    "    \n",
    "    print(keywordPair['input_text'])\n",
    "    # print(questionPair)\n",
    "    \n",
    "    # 키워드 개수와 함께 input_text를 업데이트\n",
    "    keywordCounts = keywordCounts.apply(str)\n",
    "    keywordPair['input_text'] = keywordCounts + \"<unused1>\" + keywordPair['input_text']\n",
    "    keywordPair['target_text'] = keywordPair['target_text'].apply(lambda keywords: \"<unused2>\".join(keywords))\n",
    "    \n",
    "    # print(keywordPair)\n",
    "    # print(questionPair)\n",
    "    \n",
    "    # 각 작업에 맞는 prefix 추가\n",
    "    keywordPair['input_text'] = \"키워드 추출\" + \": \" + keywordPair['input_text']\n",
    "    questionPair['input_text'] = \"질문 생성\" + \": \" + questionPair['input_text']\n",
    "    \n",
    "    return questionPair, keywordPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getTextPairFromWiki(wiki_df):\n",
    "#     # 딕셔너리 데이터를 DataFrame으로 변환\n",
    "#     if isinstance(wiki_df, dict):\n",
    "#         wiki_df = pd.DataFrame.from_dict(wiki_df, orient='index')\n",
    "        \n",
    "#     # 컬럼명 정의\n",
    "#     COLUMN_NAMES = ['input_text', 'target_text']\n",
    "    \n",
    "#     # 질문-답변 쌍 생성\n",
    "#     questionPair = pd.DataFrame([\n",
    "#         [\n",
    "#             ans['text'][0] + \"<unused0>\" + row['context'],  # 답변 + 문맥\n",
    "#             row['question']  # 질문\n",
    "#         ]\n",
    "#         for _, row in wiki_df.iterrows() if len(row['answers']['text']) > 0\n",
    "#         for ans in [row['answers']]  # 답변 추출\n",
    "#     ], columns=COLUMN_NAMES)\n",
    "    \n",
    "#     # 키워드 추출용 데이터 생성\n",
    "#     keywordPair = pd.DataFrame([\n",
    "#         [\n",
    "#             row['context'],  # 문맥\n",
    "#             list(set([ans['text'][0] for ans in [row['answers']] if len(ans['text']) > 0]))  # 중복 제거한 키워드(답변들)\n",
    "#         ]\n",
    "#         for _, row in wiki_df.iterrows()\n",
    "#     ], columns=COLUMN_NAMES)\n",
    "    \n",
    "#     # 키워드의 개수를 계산하여 3개 이상인 문단만 필터링\n",
    "#     keywordCounts = keywordPair['target_text'].apply(len)\n",
    "#     keywordPair = keywordPair[keywordCounts > 3]\n",
    "    \n",
    "#     # 키워드 개수와 함께 input_text를 업데이트\n",
    "#     keywordCounts = keywordCounts.apply(str)\n",
    "#     keywordPair['input_text'] = keywordCounts + \"<unused1>\" + keywordPair['input_text']\n",
    "#     keywordPair['target_text'] = keywordPair['target_text'].apply(lambda keywords: \"<unused2>\".join(keywords))\n",
    "    \n",
    "#     # 각 작업에 맞는 prefix 추가\n",
    "#     keywordPair['input_text'] = \"키워드 추출\" + \": \" + keywordPair['input_text']\n",
    "#     questionPair['input_text'] = \"질문 생성\" + \": \" + questionPair['input_text']\n",
    "    \n",
    "#     return questionPair, keywordPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"gogamza/kobart-base-v2\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Dataset' has no attribute 'load_from_disk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Hugging Face datasets 파일 읽기\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/train_dataset/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Pandas DataFrame으로 변환\u001b[39;00m\n\u001b[1;32m      5\u001b[0m wikipedia_df \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'load_from_disk'"
     ]
    }
   ],
   "source": [
    "# Hugging Face datasets 파일 읽기\n",
    "train = Dataset.load_from_disk('../data/train_dataset/train')\n",
    "\n",
    "# Pandas DataFrame으로 변환\n",
    "wikipedia_df = train.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...\n",
      "1       '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...\n",
      "2       강희제는 강화된 황권으로 거의 황제 중심의 독단적으로 나라를 이끌어 갔기에 자칫 전...\n",
      "3       불상을 모시기 위해 나무나 돌, 쇠 등을 깎아 일반적인 건축물보다 작은 규모로 만든...\n",
      "4       동아대학교박물관에서 소장하고 있는 계사명 사리구는 총 4개의 용기로 구성된 조선후기...\n",
      "                              ...                        \n",
      "3947    이오의 산\\n이오의 산 목록\\n 이오에는 100~150개의 산이 있다. 이들 산의 ...\n",
      "3948    애니의 고군분투 뉴욕 입성기!!\\n\\n인류학자가 꿈인 21살 소녀 '애니(스칼렛 요...\n",
      "3949    1842년에 작곡가이자 지휘자인 오토 니콜라이가 빈 궁정 오페라극장 소속 관현악단을...\n",
      "3950    원어는 고대 그리스어까지 거슬러 올라간다. 영어 문헌에 이 말이 나타나기 시작한 것...\n",
      "3951    2008년 2월 28일 실시된 2008년 함부르크 주의회 선거에서 기민련은 과반수 ...\n",
      "Name: input_text, Length: 3952, dtype: object\n"
     ]
    }
   ],
   "source": [
    "wikiQ, wikiK = getTextPairFromWiki(wikipedia_df)\n",
    "\n",
    "wikiQ = wikiQ.sample(frac=1)\n",
    "wikiK = wikiK.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wikiQ), len(wikiK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiQ = wikiQ.iloc[:len(wikiK)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wikiQ), len(wikiK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPair = pd.concat([ wikiQ, wikiK ]).sample(frac=1)\n",
    "wikiPair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengthes = wikiPair.input_text.str.len()\n",
    "target_lengthes = wikiPair.target_text.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengthes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "sns.scatterplot(x=input_lengthes, y=target_lengthes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "plt.xlim(300, 2300)\n",
    "sns.scatterplot(x=input_lengthes, y=target_lengthes)\n",
    "plt.axvline(2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPair = wikiPair[wikiPair['input_text'].str.len() < 2100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = wikiPair.input_text.apply(tokenizer.tokenize)\n",
    "target_ids = wikiPair.target_text.apply(tokenizer.tokenize)\n",
    "\n",
    "wikiPair['input_id'] = input_ids\n",
    "wikiPair['target_id'] = target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "sns.scatterplot(x=input_ids.apply(len), y=target_ids.apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPair['input_id'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "sns.scatterplot(x=wikiPair['input_id'].apply(len), y=wikiPair['target_id'].apply(len))\n",
    "plt.axvline(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(wikiPair['input_id'].apply(len) < 512).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPair = wikiPair[wikiPair['input_id'].apply(len) < 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPair['input_text'].apply(lambda text: text.split(\":\")[0].strip()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KoBARTModel = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KoBARTConditionalGeneration({\n",
    "    \"lr\": 5e-5,\n",
    "    \"gpus\": 1,\n",
    "    \"num_nodes\": 1,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"max_epochs\": 2,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_seq_len\": MAX_LENGTH\n",
    "}, tokenizer=tokenizer, model=KoBARTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = OneSourceDataModule(\n",
    "    wikiPair,\n",
    "    DatasetFromDataframe,\n",
    "    tokenizer,\n",
    "    MAX_LENGTH,\n",
    "    BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=\".\",\n",
    "    filename='model_chp/{epoch:02d}-{val_loss:.3f}',\n",
    "    verbose=True,\n",
    "    save_last=True,\n",
    "    mode='min',\n",
    "    save_top_k=-1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger(os.path.join(\".\", 'tb_logs'))\n",
    "lr_logger = pl.callbacks.LearningRateMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**({\n",
    "    \"gpus\": 1,\n",
    "    \"num_nodes\": 1,\n",
    "    \"max_epochs\": 2,\n",
    "}), logger=tb_logger, callbacks=[checkpoint_callback, lr_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = wikiPair.sample(frac=1).iloc[0].input_text\n",
    "print(input_text)\n",
    "print()\n",
    "print(model.chat(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createQuestions(row, num_questions):\n",
    "    # 문서의 context 가져오기\n",
    "    article = row['context']\n",
    "    \n",
    "    # 모델을 이용해 키워드 추출 (키워드 추출: 태스크)\n",
    "    keywords = model.chat(\"키워드 추출: \" + str(num_questions) + \"<unused1>\" + article).split(\"<unused2>\")\n",
    "    \n",
    "    # 중복 키워드를 제거하고, 양쪽 공백 제거\n",
    "    keywords = list(set([keyword.strip() for keyword in keywords]))\n",
    "    print(\"추출된 키워드:\", keywords)\n",
    "    \n",
    "    # 각 키워드에 대해 질문을 생성 (질문 생성: 태스크)\n",
    "    questions = [model.chat(\"질문 생성: \" + keyword + \"<unused0>\" + article) for keyword in keywords]\n",
    "    \n",
    "    return questions\n",
    "\n",
    "createQuestions(\" \".join(\"\"\"\n",
    "임진왜란은 1592년부터 1598년까지 2차에 걸쳐서 우리나라에 침입한 일본과의 싸움이다.\n",
    "엄청난 시련을 겪으면서도 끈질긴 저항으로 이겨내고 각성과 자기성찰을 바탕으로 민족의 운명을\n",
    "새로 개척해나간 계기가 된 전쟁이다. 명의 원조도 있었지만 승리의 가장 큰 원동력은 max으로,\n",
    "이순신에 의한 제해권의 장악과 전국에서 봉기한 의병의 활동은 불리했던 전쟁 국면을 전환시킨 결정적인\n",
    "힘이었다. 이 전란은 동아시아의 국제 정세를 크게 변화시키는 결과를 가져와, 명과 청이 교체되면서\n",
    "병자호란이라는 시련을 예고하기도 했다.\n",
    "\n",
    "조선이 임진왜란을 당하여 전쟁 초기 이를 감당하기 어려울 정도로 국력이 쇠약해진 것은\n",
    "왜란이 일어난 선조대에 이르러서 비롯된 것은 아니었다. 이미 훨씬 이전부터 중쇠의 기운이\n",
    "나타나기 시작하였다.정치적으로는 연산군 이후 명종대에 이르는 4대 사화와 훈구·사림 세력간에\n",
    "계속된 정쟁으로 인한 중앙 정계의 혼란, 사림 세력이 득세한 선조 즉위 이후 격화된 당쟁\n",
    "등으로 정치의 정상적인 운영을 수행하기 어려운 지경이었다.군사적으로도 조선 초기에 설치된\n",
    "국방체제가 붕괴되어 외침에 대비하기 위한 방책으로 군국기무를 장악하는 비변사라는 합의 기관을\n",
    "설치했으나, 이것 또한 정상적인 기능을 발휘하지 못하였다.이이는 남왜북호의\n",
    "침입에 대처하기 위하여 십만양병설을 주장하기도 하였다. 그러나 국가 재정의 허약으로\n",
    "뜻을 이루지 못하고, 사회는 점점 해이해지고 문약에 빠져 근본적인 국가 방책이 확립되지\n",
    "못한 실정이었다.이러할 즈음 일본에서는 새로운 형세가 전개되고 있었다. 즉, 15세기 후반\n",
    "서세동점에 따라 일본에는 유럽 상인들이 들어와 신흥 상업 도시가 발전되어 종래의 봉건적인 지배\n",
    "형태가 위협받기 시작하였다.\n",
    "\"\"\".strip().split(\"\\n\")), 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
